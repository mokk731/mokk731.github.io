https://www.cheshirex.com/9019.html
DeepSeek-R1:14B 未受限版本 模型本地部署教程（N卡+A卡）

DeepSeek深度求索官网：https://www.deepseek.com
Ollama官网：https://ollama.com/
Chatbox官网：https://chatboxai.app/zh
DeepSeek-R1-Distill-Qwen-14B-abliterated-v2未受限模型：
https://huggingface.co/huihui-ai/DeepSeek-R1-Distill-Qwen-14B-abliterated-v2




模型名称	                                  建议显存	
DeepSeek-R1-Distill-Qwen-1.5B                 	6G及以下	
DeepSeek-R1-Distill-Qwen-7B                   	8G及以下	
DeepSeek-R1-Distill-Llama-8B                  	8G以上	
DeepSeek-R1-Distill-Qwen-14B	                12到16G	
DeepSeek-R1-Distill-Qwen-32B	                24G以上	
DeepSeek-R1-Distill-Llama-70B                 	双14G显存
DeepSeek-R1-Distill-Llama-671B	                500G以上





# 部署步骤

1.下载安装Ollama程序

打开Ollama官网直接下载，自己是什么系统就下什么安装包。
文件大概745MB，国内直接下载速度可能比较慢，可以开梯子下载。
下载完成直接打开安装包安装，安装不可以选安装目录，默认安装到C盘。

2.设置环境变量

一般C盘没那么多空间给我们存放模型文件，这里本文将变量目录指向了D盘，自己根据实际需求设置，目录名随便。
变量名：OLLAMA_MODELS
变量值：D:\deepseek 你要存放模型的目录，可以根据自己磁盘实际情况随便创建个目录。
重要：设置完成后重启电脑，否则直接开始使用会将文件下载到C盘。

3.安装模型

本文安装的是14B的未受限模型，使用下面命令在CMD运行就可以了。
下载过成可能前面速度快，到了后面比较慢，耐心等待吧。
ollama run huihui_ai/deepseek-r1-abliterated:14b
ollama run huihui_ai/deepseek-r1-abliterated:32b
ollama run huihui_ai/deepseek-r1-abliterated:70b


# 下载安装Chatbox

Chatbox可以直接在官网免费下载：https://chatboxai.app/zh
安装Chatbox后我们可以通过常见的对话框来使用deepseek。
安装后运行Chatbox，我们选择是用自己的本地模型
然后选择本地模型，请注意这里选择Ollama API，不要选择deepseek API。
我们本地模型是通过Ollama来提供的。


------------------------------------------------------



DeepSeek 32B模型的本地部署对GPU显存的需求较高，具体显存大小取决于是否采用量化技术以及具体的硬件配置。以下是一些关键信息：

未量化时的显存需求

基础显存需求：如果以半精度（FP16）计算，DeepSeek 32B模型的基础显存需求约为64GB，
但如果考虑安全系数、上下文扩展量和系统缓存等因素，总显存需求可能会达到90.2GB。
推荐配置：在未量化的情况下，单卡显存低于90.2GB可能无法稳定运行，因此通常需要使用多卡分布式架构。例如，可以使用双A100 40GB显卡，或者四张RTX 4090 24GB显卡


量化后的显存需求

    8位量化（INT8）：如果采用8位量化，显存需求可以降至32GB，此时单张显存32GB及以上的GPU可以满足需求。             ###########

    4位量化（INT4）：如果采用4位量化，显存需求可以进一步降至16GB，此时单张显存16GB及以上的GPU可以尝试部署。

总结

    如果不使用量化技术，建议使用显存至少为40GB的GPU（如A100 40GB），或者多张显存较小的GPU进行并行计算。
    如果采用量化技术，显存需求可以显著降低，例如8位量化后需要32GB显存，4位量化后仅需16GB显存。


